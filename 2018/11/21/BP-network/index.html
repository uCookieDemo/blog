<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.0.1',
    sidebar: {"position":"left","display":"always","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="BP神经网络 写在前面：BP神经网络模型在我研究生期间上课学习过，当时只是学习了原理，并没有实现具体计算过程   一、神经元模型神经元，也叫做感知器，是神经网络中计算的基本单元，一个典型的神经元结构如图所示：   图1 神经元结构图    输入向量 $(x_1,x_2,…,x_n)^T$,每一个输入上还有一个对应权值向量$W=(w_1,w_2,…,w_n)^T$,此外还一个偏置项$b$, 激活函数">
<meta name="keywords" content="机器学习,神经网络,误差反向传播">
<meta property="og:type" content="article">
<meta property="og:title" content="BP-network">
<meta property="og:url" content="cookiedemo.netlify.com/2018/11/21/BP-network/index.html">
<meta property="og:site_name" content="CookieDemo的学习记录">
<meta property="og:description" content="BP神经网络 写在前面：BP神经网络模型在我研究生期间上课学习过，当时只是学习了原理，并没有实现具体计算过程   一、神经元模型神经元，也叫做感知器，是神经网络中计算的基本单元，一个典型的神经元结构如图所示：   图1 神经元结构图    输入向量 $(x_1,x_2,…,x_n)^T$,每一个输入上还有一个对应权值向量$W=(w_1,w_2,…,w_n)^T$,此外还一个偏置项$b$, 激活函数">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/26/FAnXtA.jpg">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<meta property="og:image" content="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<meta property="og:updated_time" content="2019-03-14T07:35:10.611Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BP-network">
<meta name="twitter:description" content="BP神经网络 写在前面：BP神经网络模型在我研究生期间上课学习过，当时只是学习了原理，并没有实现具体计算过程   一、神经元模型神经元，也叫做感知器，是神经网络中计算的基本单元，一个典型的神经元结构如图所示：   图1 神经元结构图    输入向量 $(x_1,x_2,…,x_n)^T$,每一个输入上还有一个对应权值向量$W=(w_1,w_2,…,w_n)^T$,此外还一个偏置项$b$, 激活函数">
<meta name="twitter:image" content="https://s1.ax1x.com/2018/11/26/FAnXtA.jpg">






  <link rel="canonical" href="cookiedemo.netlify.com/2018/11/21/BP-network/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>BP-network | CookieDemo的学习记录</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">CookieDemo的学习记录</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">程序元萌新</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="cookiedemo.netlify.com/2018/11/21/BP-network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CookieDemo">
      <meta itemprop="description" content="机器学习 | 嵌入式 | 理工男">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CookieDemo的学习记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">BP-network

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-11-21 13:03:36" itemprop="dateCreated datePublished" datetime="2018-11-21T13:03:36+08:00">2018-11-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-03-14 15:35:10" itemprop="dateModified" datetime="2019-03-14T15:35:10+08:00">2019-03-14</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="BP神经网络"><a href="#BP神经网络" class="headerlink" title="BP神经网络"></a>BP神经网络</h1><blockquote>
<p>写在前面：BP神经网络模型在我研究生期间上课学习过，当时只是学习了原理，并没有实现具体计算过程</p>
</blockquote>
<hr>
<h2 id="一、神经元模型"><a href="#一、神经元模型" class="headerlink" title="一、神经元模型"></a>一、神经元模型</h2><p>神经元，也叫做感知器，是神经网络中计算的基本单元，一个典型的神经元结构如图所示：</p>
<div align="center">
<img src="https://s1.ax1x.com/2018/11/26/FAnXtA.jpg">
<p>图1 神经元结构图</p>
</div>

<ul>
<li><strong>输入向量</strong> $(x_1,x_2,…,x_n)^T$,每一个输入上还有一个对应权值向量$W=(w_1,w_2,…,w_n)^T$,此外还一个偏置项$b$,</li>
<li><strong>激活函数</strong> 神经元的激活函数$f(x)$可以有很多选择，比如阶跃函数，sigmoid函数，tanh函数等。</li>
<li><strong>输出</strong>一个神经元的输出计算如下：<script type="math/tex; mode=display">output=f(w_1x_1+w_2x_2+...+w_nx_n+b)</script>实际计算的时候我们很少使用这样的式子来计算，我们的做法是，在输入向量的最前面添加一个1，输入向量变为<script type="math/tex; mode=display">X=(1,x_1,x_2,...,x_n)^T</script>另$w_0=b$添加到权值向量中变为<script type="math/tex; mode=display">W=(w_0,w_1,w_2,...,w_n)^T</script>这样我们就可以将神经元的计算写成为矩阵计算的形式<script type="math/tex">output = f(W^TX)</script></li>
</ul>
<hr>
<h2 id="二、神经网络"><a href="#二、神经网络" class="headerlink" title="二、神经网络"></a>二、神经网络</h2><h3 id="2-1神经网络结构—-全连接神经网络"><a href="#2-1神经网络结构—-全连接神经网络" class="headerlink" title="2.1神经网络结构—-全连接神经网络"></a>2.1神经网络结构—-全连接神经网络</h3><p>神经网络实际上就是按照一定的规则连接起来的多个神经元。如下图，图2所示，展示了一个全连接的三层神经网络结构</p>
<blockquote>
<p>神经网络介绍及图参考博客 <a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="noopener">https://www.zybuluo.com/hanbingtao/note/476663</a></p>
</blockquote>
<div align="center">
<img src="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<p>图2 标记的神经元结构图</p>
</div>


<ul>
<li>神经元按照<strong>层</strong>结构来布局，分为输入层，隐藏层，输出层。</li>
<li>同一层神经元之间没有连接。</li>
<li>第N层的神经元和第N-1层的所有神经元相连，第N-1层神经元的输出就是第N层神经元的输入。</li>
<li>每一个连接代表一个权值。</li>
</ul>
<h3 id="2-2-全连接网络的网络的输出"><a href="#2-2-全连接网络的网络的输出" class="headerlink" title="2.2 全连接网络的网络的输出"></a>2.2 全连接网络的网络的输出</h3><p>神经网络我们也可以理解成为一个复杂的不知道具体函数解析式的函数，即从出入向量X到输出向量Y的映射关系：<br>在神经网络中我们可以根据输入向量以及其连接方式一层一层的计算各层的输出，最终来计算网络的输出。为了方便说明我们将上图中给每个单元编号，得到图3：</p>
<div align="center">
<img src="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<p>图3 标记的神经元结构图</p>
</div>


<blockquote>
<p>note：有的教程中不认为输出数据作为一层这里我们将输出数据看为神经网络的一层,即输入层的结果就是输入向量X</p>
</blockquote>
<p>接下来我们按照神经元的计算方式分别计算a4，a5，a6，a7神经元的输出,即计算隐藏层的输出，这里我们激活函数为$f(x)=sigmoid(x)$：</p>
<script type="math/tex; mode=display">
  a_4=f(w_{40}+w_{41}x1+w_{42}x_2+w_{43}x_3)\\
  a_5=f(w_{50}+w_{51}x1+w_{52}x_2+w_{53}x_3)\\
  a_6=f(w_{60}+w_{61}x1+w_{62}x_2+w_{63}x_3)\\
  a_7=f(w_{70}+w_{71}x1+w_{72}x_2+w_{73}x_3)\\</script><p>我们同样可以利用计算神经元的方法将隐藏层的计算矩阵化，我以前的线性代数学的不好，所以经常搞错矩阵的维数问题，所以我在矩阵下面加了维度。</p>
<script type="math/tex; mode=display">\begin{bmatrix}
 w_{40} & w_{41}  & w_{42} & w_{43} \\
 w_{50} & w_{51}  & w_{52} & w_{53} \\
 w_{60} & w_{61}  & w_{62} & w_{63} \\
 w_{70} & w_{71}  & w_{72} & w_{73} \\
\end{bmatrix}_{4\times4} 
\times
\begin{bmatrix}
 1\\ x_1\\ x_2\\ x_3
\end{bmatrix}_{4\times1}=
\begin{bmatrix}
 a_1\\ a_2\\ a_3\\ a_4\\
\end{bmatrix}_{4\times1}</script><p>这样我么就以把隐藏层的输出用矩阵表示出来(注意这里实际都是列向量，注意上面的转置符号)：</p>
<script type="math/tex; mode=display">
 W_4=\begin{bmatrix} w_{40} & w_{41} & w_{42} & w_{43} \end{bmatrix}^T\\
 W_5=\begin{bmatrix} w_{50} & w_{51} & w_{52} & w_{53} \end{bmatrix}^T\\
 W_6=\begin{bmatrix} w_{60} & w_{61} & w_{62} & w_{63} \end{bmatrix}^T\\
 W_7=\begin{bmatrix} w_{70} & w_{71} & w_{72} & w_{73} \end{bmatrix}^T\\
--\\
 W = \begin{bmatrix} W_4^T & W_5^T & W_6^T & W_7^T \end{bmatrix}^T\\
--\\
 X = \begin{bmatrix} 1 & x_1 & x_2 & x_3 & x_4 \end{bmatrix}^T\\
--\\
 A = \begin{bmatrix} a_1 & a_2 & a_3 & a_4 \end{bmatrix}^T</script><p>这里我们可以观察到，对于隐藏层来说，其所有的权值可以由矩阵表示，且该矩阵的维数为（前一本层节点个数$\times$前一层的节点个数+1 ），这里维数都加一的原因是我们把偏置项一起放入权重矩阵的结果，所以最终的隐藏的输出可以表示为：</p>
<script type="math/tex; mode=display">A=f(W \cdot X)</script><p>同理我们直接给出隐藏层到输出层的计算方法，隐藏层输出为 </p>
<script type="math/tex; mode=display">A = \begin{bmatrix} 1 & a_1 & a_2 & a_3 & a_4 \end{bmatrix}^T</script><p>隐藏层到输出层的权值矩阵为</p>
<script type="math/tex; mode=display">V_{2\times5}=\begin{bmatrix}
  v_{80} & v_{81}  & v_{82} & v_{83} & v_{84}\\
  v_{90} & v_{91}  & v_{92} & v_{93} & v_{24}\\
\end{bmatrix}_{2\times5}</script><p>输出<script type="math/tex">Y=f(V \cdot A)=\begin{bmatrix} y_1 & y_2 \end{bmatrix}^T</script></p>
<hr>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>反向传播算实际上是链式求导法则的一个应用，理解起来不是很难（前提你还记得高数里的复合函数链式求导法则），按照机器学习的通用套路，我们首先确定神经网络的目标函数，然后使用熟悉的梯度下降算法去求解目标函数的最小值的参数值。<br>我们选取网络所有输出层节点的误差平方和作为目标输出函数：</p>
<script type="math/tex; mode=display">E_d=\frac{1}{2}\sum_{i\in outpus}(t_i-y_i)^2\tag{1}</script><p>其中$E_d$表示样本$d$的误差，然后我们使用之前记录过的梯度下降法来对目标函数进行优化：</p>
<script type="math/tex; mode=display">w_{ji} \leftarrow w_{ji}-\alpha \frac{\partial E_d}{\partial w_{ji}} \tag{2}</script><div align="center">
<img src="https://s1.ax1x.com/2018/11/26/FAnjfI.png">
<p>图2 标记的神经元结构图</p>
</div>

<p>观察上图，我们发现权重$w_{ji}$仅能通过节点j的输入值影响网络的其他部分，设$net_j$是节点j的<strong>加权输入</strong>，即：</p>
<script type="math/tex; mode=display">net_j=w_j \cdot x_j=\sum_i w_{ji}x_{ji} \tag{3}</script><p>$E<em>d$是$net_j$的函数，而$net_j$是$w</em>{ji}$的函数，根据链式求导法则得：</p>
<script type="math/tex; mode=display">
\frac{\partial E_d}{\partial w_{ji}}=\frac{\partial E_d}{\partial net_j} 
\frac{\partial net_j}{\partial w_{ji}} \tag{4}</script><script type="math/tex; mode=display">
\frac{\partial E_d}{\partial w_{ji}}=\frac{\partial E_d}{\partial net_j} 
\frac{\partial \sum_iw_{ji}x_{ji}}{\partial w_{ji}} \tag{5}</script><script type="math/tex; mode=display">
\frac{\partial E_d}{\partial w_{ji}}=\frac{\partial E_d}{\partial net_j}x_{ji}\tag{6}</script><p>(6)式中，$x_{ji}$是节点 $i$ 传递给节点 $j$ 的输入值，也是就节点 $i$ 的输入值。</p>
<p><strong><em>对于$\frac{\partial E_d}{\partial net_j}$的推导</em></strong>，需要区分<strong>输出层</strong>和<strong>隐藏层</strong>的两种情况来说</p>
<h4 id="输出层权值的训练"><a href="#输出层权值的训练" class="headerlink" title="输出层权值的训练"></a>输出层权值的训练</h4><p>对于<strong>输出层</strong>来说，$net_j$ 仅能通过节点 $j$ 的的输出值 $y_j$ 来影响网络的其他部分，也就是说 $E_d$ 是 $y_j$ 的函数，其中 $y_j=sigmoid(net_j)$ 。所以我们可以再次使用链式求导法则：</p>
<blockquote>
<p>这里利用了sigmoid函数的求导的性质，另$y=sigmoid(x)$，则$y^\prime=y(i-y)$，这里我的博客在梯度下降算法里面有推导</p>
</blockquote>
<script type="math/tex; mode=display">
\frac{\partial E_d}{\partial net_j}=\frac{\partial E_d}{\partial y_j} 
\frac{\partial y_j}{\partial net_j} \tag{7}</script><p>考虑（7）式的第一项，</p>
<script type="math/tex; mode=display">
\frac{\partial E_d}{\partial y_j}=\frac{\partial}{\partial y_j} \frac{1}{2}
\sum_{i \in outputs}(t_i-y_i)^2 \\
=\frac{1}{2} \frac{\partial}{\partial y_j} (t_i-y_i)^2 \tag{8}</script><p>考虑（7）式的第二项，</p>
<script type="math/tex; mode=display">
\frac{\partial y_i}{\partial net_j}=\frac{\partial sigmod(net_j)}{\partial net_j}=y_j(1-y_j)</script><p>如果另$\delta_j=-\frac{\partial E_d}{\partial net_j}$,也就是节点的误差项 $\delta_j$ 网络误差对这个节点输入的偏导数的相反项,带入（7）式得:</p>
<script type="math/tex; mode=display">\delta_j=(t_j-y_j)y_j(1-y_j)\tag{9}</script><p>我们将上述推导公式（9）（6）式代入梯度下降公式（2）得：</p>
<script type="math/tex; mode=display">w_{ji}\leftarrow w_{ji}-\alpha \frac{\partial E_d}{\partial w_{ji}}\\
=w_{ji}+\alpha(t_j-y_j)y_j(1-y_j)x_{ji}\\
= w_{ji}+\alpha\delta_jx_{ji} \tag{10}</script><h4 id="隐藏层权值得训练"><a href="#隐藏层权值得训练" class="headerlink" title="隐藏层权值得训练"></a>隐藏层权值得训练</h4><p>现在我们要推导出隐藏层得$\frac{\partial E_d}{\partial net_j}$。<br>首先，我们需要定义节点 $j$ 得所有直接下游节点集合$DownStream(j)$。例如节点4来说，他直接下游节点是节点8，节点9。我们可以节点 $j$ 只能通过影响$DownStream(j)$进而影响到$E_d$。我们设$net_k$节点 $j$ 得下游输入,则$E_d$是$net_k$的函数，而$net_k$是$net_j$的函数。因为$net_k$有多个,我们应用全导数公式,可以做出如下推导。</p>
<script type="math/tex; mode=display">\frac{\partial E_d}{\partial net_j}=\sum_{k \in Downstream(j)}\frac{\partial E_d}{\partial net_k}\frac{\partial net_k}{\partial net_j}</script><script type="math/tex; mode=display">=\sum_{k \in Downstream(j)}-\delta_k \cdot \frac{\partial net_k}{\partial net_j}</script><script type="math/tex; mode=display">=\sum_{k \in Downstream(j)}-\delta_k\frac{\partial net_k}{\partial a_j}\frac{\partial a_j}{\partial net_j}\\</script><script type="math/tex; mode=display">=\sum_{k \in Downstream(j)}-\delta_k w_{kj}\frac{\partial a_j}{\partial net_j}</script><script type="math/tex; mode=display">=\sum_{k \in Downstream(j)}-\delta_k w_{kj}a_j(1-a_j)\tag{11}</script><script type="math/tex; mode=display">=-a_j(1-a_j)\sum_{k \in Downstream(j)}\delta_k w_{kj}\tag{12}</script><p>最终归纳可得：</p>
<ul>
<li>对于输出层节点 $i$ 来说, $t_i$ 为目标输出，$y_i$为网络计算输出，$\delta_i$为该节点的误差项。<script type="math/tex; mode=display">\delta_i=y_i(1-y_i)(t_i-y_i) \tag{式1}</script>带入公式分别可以得到节点8和节点9的误差项<script type="math/tex; mode=display">\delta_8=y_1(1-y_1)(t_1-y_1)\\
\delta_9=y_2(1-y_2)(t_2-y_2)\tag{式2}</script>根据输出层权值更新公式：<script type="math/tex; mode=display">w_{ji} = w_{ji} - \alpha\delta_jx_{ji}</script>可以得到隐藏层到输出层的权重更新公式：<script type="math/tex; mode=display">\begin{bmatrix}  
w_{80}  & w_{84} & w_{85} & w_{86} & w_{87} \\
w_{90}  & w_{94} & w_{95} & w_{96} & w_{87} \\
\end{bmatrix} - \alpha\begin{bmatrix}  
\delta_8a_0 & \delta_8a_4 & \delta_8a_5 & \delta_8a_6 & \delta_8a_7 \\
\delta_9a_0 & \delta_9a_4 & \delta_9a_5 & \delta_9a_6 & \delta_9a_7 \\
\end{bmatrix}\\</script><script type="math/tex; mode=display">\begin{bmatrix}  
w_{80}  & w_{84} & w_{85} & w_{86} & w_{87} \\
w_{90}  & w_{94} & w_{95} & w_{96} & w_{87} \\
\end{bmatrix} - 
\alpha
\begin{bmatrix} \delta_8\\ \\ \delta_9\end{bmatrix}
\begin{bmatrix} a_0 & a_4 & a_5 & a_6 & a_7  \end{bmatrix}</script></li>
</ul>
<p>因此得到矩阵表示式为(通常实际编程的时候需要利用矩阵计算)：</p>
<script type="math/tex; mode=display">W := W - \alpha\ \delta X^T</script><ul>
<li>对于隐藏层节点来说，<script type="math/tex; mode=display">\delta_i= a_i(1-a_i)\sum_{k \in outputs}w_{ki}\delta_k</script>其中 $a<em>i$ 是节点 $i$ 的输出值，$w</em>{ki}$ 是节点 i 到节点 k 的连接权重，$\delta_k$ 是节i到下一层节点 k 的误差项。<br>带入公式我么可以分别得到节点4、节点5、节点6和节点7的误差项，<script type="math/tex; mode=display">\delta_4 = a_4(1-a_4)(w_{84}\delta_8 + w_{94}\delta_9)\\
\delta_5 = a_5(1-a_5)(w_{85}\delta_8 + w_{95}\delta_9)\\
\delta_6 = a_6(1-a_6)(w_{86}\delta_8 + w_{96}\delta_9)\\
\delta_7 = a_7(1-a_7)(w_{87}\delta_8 + w_{97}\delta_9)\\</script></li>
</ul>
<p>这里我们可以得到误差的矩阵计算方法，在实际编程计算的时候，前面部分的$a(1-a)$这一块可以直接写的，是向量对应元素的运算。后面的计算可以统一为矩阵运算，</p>
<script type="math/tex; mode=display">\begin{bmatrix}  
  w_{84}\delta_8 + w_{94}\delta_9 \\
  w_{85}\delta_8 + w_{95}\delta_9 \\
  w_{86}\delta_8 + w_{96}\delta_9 \\
  w_{87}\delta_8 + w_{97}\delta_9 \\
  \end{bmatrix}_{4 \times 1} = 
  \begin{bmatrix}  
  w_{84} & w_{94} \\
  w_{85} & w_{95} \\
  w_{86} & w_{96} \\
  w_{87} & w_{97} \\
  \end{bmatrix}_{4 \times 2} \cdot
  \begin{bmatrix}  
  \delta_8 \\
  \delta_9 \\
  \end{bmatrix}_{2 \times 1}</script><p>  类似的根据权重更新方法，我们可以得到输入层到隐藏层的权重更新公式：</p>
<script type="math/tex; mode=display">\begin{bmatrix} 
    w_{40} & w_{41} & w_{42} & w_{43} \\
    w_{50} & w_{51} & w_{52} & w_{53} \\
    w_{60} & w_{61} & w_{62} & w_{63} \\
    w_{70} & w_{71} & w_{72} & w_{73} \\
    \end{bmatrix}_{4 \times 4} + \alpha
    \begin{bmatrix} 
    \delta_4 & \delta_4x_1 & \delta_4x_2 & \delta_4x_3 \\
    \delta_5 & \delta_5x_1 & \delta_5x_2 & \delta_5x_3 \\
    \delta_6 & \delta_6x_1 & \delta_6x_2 & \delta_6x_3 \\
    \delta_7 & \delta_7x_1 & \delta_7x_2 & \delta_7x_3 \\
    \end{bmatrix}_{4 \times 4} \\</script><script type="math/tex; mode=display">
    =\begin{bmatrix} 
    w_{40} & w_{41} & w_{42} & w_{43} \\
    w_{50} & w_{51} & w_{52} & w_{53} \\
    w_{60} & w_{61} & w_{62} & w_{63} \\
    w_{70} & w_{71} & w_{72} & w_{73} \\
    \end{bmatrix}_{4 \times 4} + \alpha 
    \begin{bmatrix} \delta_4 \\ \delta_5 \\ \delta_6 \\ \delta_7 \end{bmatrix}
    \cdot \begin{bmatrix} 1 & x_1 & x_2 & x_3 \end{bmatrix}</script><p>到此为止BP网络的向量更新部分，以及编程中的矩阵化更新完毕。</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>CookieDemo</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    
    <a href="/cookiedemo.netlify.com/2018/11/21/BP-network/" title="BP-network">cookiedemo.netlify.com/2018/11/21/BP-network/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/神经网络/" rel="tag"># 神经网络</a>
          
            <a href="/tags/误差反向传播/" rel="tag"># 误差反向传播</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/11/15/nuc970-yaffs2文件系统启动的配置工作/" rel="next" title="nuc970-yaffs2文件系统启动的配置工作">
                <i class="fa fa-chevron-left"></i> nuc970-yaffs2文件系统启动的配置工作
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/11/29/regularization-term/" rel="prev" title="正则化-解决学习中的过拟合和欠拟合问题">
                正则化-解决学习中的过拟合和欠拟合问题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">CookieDemo</p>
              <div class="site-description motion-element" itemprop="description">机器学习 | 嵌入式 | 理工男</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">15</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:yourname@gmail.com" title="E-Mail &rarr; mailto:yourname@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#BP神经网络"><span class="nav-number">1.</span> <span class="nav-text">BP神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、神经元模型"><span class="nav-number">1.1.</span> <span class="nav-text">一、神经元模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、神经网络"><span class="nav-number">1.2.</span> <span class="nav-text">二、神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1神经网络结构—-全连接神经网络"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1神经网络结构—-全连接神经网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-全连接网络的网络的输出"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 全连接网络的网络的输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播算法"><span class="nav-number">1.2.3.</span> <span class="nav-text">反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#输出层权值的训练"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">输出层权值的训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐藏层权值得训练"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">隐藏层权值得训练</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CookieDemo</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/affix.js?v=7.0.1"></script>

  <script src="/js/src/schemes/pisces.js?v=7.0.1"></script>




  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src>
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
